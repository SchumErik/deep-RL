{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imitation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook uses imitation learning (behavioral cloning) with a number of the openAI gym environments.  With imitation learning, you take an expert policy and record its inputs and outputs. You then use an algorithm to \"imitate\" it.  As you will see this works with varying degrees of success, but not very well.  The problem with this method is that the errors accumulate.  See http://rll.berkeley.edu/deeprlcourse/docs/week_2_lecture_1_behavior_cloning.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook is largely based off code posted at https://github.com/ghostFaceKillah/deep-rl-berkeley/tree/master/hw1 and I also used https://github.com/favetelinguis/DeepReinforcementLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is setup to give you an idea of how the algorithms are working.  GhostFaceKillah has a nice set of scripts that will run all the environments with different conditions and provide results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "My suggestion is to pick one task.  Then set the epochs to 1 and go through and create the data from the expert policy.  Then build a model to imitate the policy.  You can then save the outputs of this policy.  Finally, you can view both the expert policy (loaded as data) and your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import load_policy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tf_util\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-14 11:49:33,806] Making new env: Reacher-v1\n"
     ]
    }
   ],
   "source": [
    "task = 'Reacher-v1'\n",
    "#task = 'Ant-v1'\n",
    "num_rollouts =  5  ###Important parameter for how much data you are collecting/running\n",
    "use_cached_data_for_training = True\n",
    "cached_data_path = \"data/\" + task + \"-their.p\"    \n",
    "their_data_path = \"data/\" + task + \"-their.p\"\n",
    "our_data_path = \"data/\" + task + \"-our.p\"\n",
    "expert_policy_file = \"experts/\" + task + \".pkl\"\n",
    "\n",
    "env = gym.make(task)\n",
    "max_steps = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "envname = task\n",
    "render_them =  False\n",
    "render_us = False\n",
    "\n",
    "# neural net params\n",
    "learning_rate = 0.001\n",
    "epochs = 50   ###important parameter for how long you are training your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_data_table_stats(data):\n",
    "    mean = data['returns'].mean()\n",
    "    std = data['returns'].std()\n",
    "    x = data['steps']\n",
    "    pct_full_steps =  (x / x.max()).mean()\n",
    "\n",
    "    return pd.Series({\n",
    "        'mean reward': mean,\n",
    "        'std reward': std,\n",
    "        'pct full rollout': pct_full_steps\n",
    "    })\n",
    "\n",
    "def view_data(data,rollouts):\n",
    "    returns = []\n",
    "    observations = []\n",
    "    actions = []\n",
    "    print (\"Total rollouts from data: \", rollouts)\n",
    "    env = gym.make(envname)\n",
    "    for i in range(rollouts):\n",
    "        print (\"Start rollout \", i)\n",
    "        observation = env.reset()\n",
    "        steps = 0\n",
    "        for t in range(2000):\n",
    "            env.render()\n",
    "            x = t + i*max_steps\n",
    "            action = data['actions'][x,:,:]\n",
    "            observations.append(t)\n",
    "            actions.append(action)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            steps += 1\n",
    "            if steps >= max_steps:\n",
    "                print(\"Max timestep reached\")\n",
    "                break\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                #env.render(close=True)\n",
    "                break\n",
    "                \n",
    "def view_model(model,rollouts):\n",
    "    returns = []\n",
    "    observations = []\n",
    "    actions = []\n",
    "    env = gym.make(envname)\n",
    "    print (\"Total rollouts from model: \", rollouts)\n",
    "    for i in range(rollouts):\n",
    "        #observation = env.reset()\n",
    "        print (\"Start rollout \", i)\n",
    "        obs = env.reset()\n",
    "        steps = 0\n",
    "        for t in range(2000):\n",
    "            env.render()\n",
    "            #print(observation)\n",
    "            action = model.predict(obs[None, :])\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            steps += 1\n",
    "            if steps >= max_steps:\n",
    "                print(\"Max timestep reached\")\n",
    "                break\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from expert policy (only need to run this once per environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to get a list of all states and the corresponding actions that the expert performed. You will want plenty of data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 35.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering expert data\n",
      "loading and building expert policy\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built\n",
      "Total rollouts for building policy:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Gathering expert data')\n",
    "print('loading and building expert policy')\n",
    "policy_fn = load_policy.load_policy(expert_policy_file)\n",
    "print('loaded and built')\n",
    "\n",
    "with tf.Session():\n",
    "    tf_util.initialize()\n",
    "\n",
    "    max_steps = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "    print (\"Total rollouts for building policy: \", num_rollouts)\n",
    "    returns = []\n",
    "    observations = []\n",
    "    actions = []\n",
    "    steps_numbers = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(num_rollouts)):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        totalr = 0.\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = policy_fn(obs[None,:])\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            totalr += r\n",
    "            steps += 1\n",
    "            if render_them:\n",
    "                env.render()\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "        steps_numbers.append(steps)\n",
    "        returns.append(totalr)\n",
    "\n",
    "    expert_data = {'observations': np.array(observations),\n",
    "                   'actions': np.array(actions),\n",
    "                   'returns': np.array(returns),\n",
    "                   'steps': np.array(steps_numbers)}\n",
    "\n",
    "pickle.dump(expert_data, open(their_data_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from expert policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##load data from file\n",
    "data = pickle.load(open(cached_data_path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model -- there are different models available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is going to take the states as inputs and actions as the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Alternative model designs:\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_inputs/2, input_dim=num_inputs, init='normal', activation='relu'))\n",
    "    model.add(Dense(num_outputs, init='normal'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model\n",
    "def regularized_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=num_inputs, init='normal', activation='relu',W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01),b_regularizer=l2(0.01)))\n",
    "    model.add(Dense(64, input_dim=num_inputs, init='normal', activation='relu',W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01),b_regularizer=l2(0.01)))\n",
    "    model.add(Dense(num_outputs, init='normal',W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01),b_regularizer=l2(0.01)))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "def wide_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=num_inputs, init='normal', activation='relu'))\n",
    "    model.add(Dense(num_outputs, init='normal'))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def awesome_model():\n",
    "    model = Sequential([\n",
    "    Lambda(lambda x: (x - mean) / std, batch_input_shape=(None, observations_dim)),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(actions_dim)])\n",
    "\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 225 samples, validate on 25 samples\n",
      "Epoch 1/50\n",
      "0s - loss: 0.0080 - mean_absolute_error: 0.0476 - val_loss: 0.0087 - val_mean_absolute_error: 0.0472\n",
      "Epoch 2/50\n",
      "0s - loss: 0.0080 - mean_absolute_error: 0.0476 - val_loss: 0.0087 - val_mean_absolute_error: 0.0470\n",
      "Epoch 3/50\n",
      "0s - loss: 0.0080 - mean_absolute_error: 0.0477 - val_loss: 0.0086 - val_mean_absolute_error: 0.0471\n",
      "Epoch 4/50\n",
      "0s - loss: 0.0079 - mean_absolute_error: 0.0478 - val_loss: 0.0085 - val_mean_absolute_error: 0.0473\n",
      "Epoch 5/50\n",
      "0s - loss: 0.0079 - mean_absolute_error: 0.0480 - val_loss: 0.0085 - val_mean_absolute_error: 0.0475\n",
      "Epoch 6/50\n",
      "0s - loss: 0.0079 - mean_absolute_error: 0.0482 - val_loss: 0.0084 - val_mean_absolute_error: 0.0477\n",
      "Epoch 7/50\n",
      "0s - loss: 0.0078 - mean_absolute_error: 0.0484 - val_loss: 0.0084 - val_mean_absolute_error: 0.0479\n",
      "Epoch 8/50\n",
      "0s - loss: 0.0078 - mean_absolute_error: 0.0486 - val_loss: 0.0084 - val_mean_absolute_error: 0.0480\n",
      "Epoch 9/50\n",
      "0s - loss: 0.0078 - mean_absolute_error: 0.0487 - val_loss: 0.0083 - val_mean_absolute_error: 0.0481\n",
      "Epoch 10/50\n",
      "0s - loss: 0.0078 - mean_absolute_error: 0.0488 - val_loss: 0.0083 - val_mean_absolute_error: 0.0482\n",
      "Epoch 11/50\n",
      "0s - loss: 0.0077 - mean_absolute_error: 0.0489 - val_loss: 0.0083 - val_mean_absolute_error: 0.0481\n",
      "Epoch 12/50\n",
      "0s - loss: 0.0077 - mean_absolute_error: 0.0488 - val_loss: 0.0083 - val_mean_absolute_error: 0.0481\n",
      "Epoch 13/50\n",
      "0s - loss: 0.0077 - mean_absolute_error: 0.0487 - val_loss: 0.0083 - val_mean_absolute_error: 0.0480\n",
      "Epoch 14/50\n",
      "0s - loss: 0.0077 - mean_absolute_error: 0.0486 - val_loss: 0.0082 - val_mean_absolute_error: 0.0479\n",
      "Epoch 15/50\n",
      "0s - loss: 0.0077 - mean_absolute_error: 0.0485 - val_loss: 0.0082 - val_mean_absolute_error: 0.0479\n",
      "Epoch 16/50\n",
      "0s - loss: 0.0076 - mean_absolute_error: 0.0483 - val_loss: 0.0082 - val_mean_absolute_error: 0.0478\n",
      "Epoch 17/50\n",
      "0s - loss: 0.0076 - mean_absolute_error: 0.0481 - val_loss: 0.0082 - val_mean_absolute_error: 0.0477\n",
      "Epoch 18/50\n",
      "0s - loss: 0.0076 - mean_absolute_error: 0.0480 - val_loss: 0.0082 - val_mean_absolute_error: 0.0476\n",
      "Epoch 19/50\n",
      "0s - loss: 0.0076 - mean_absolute_error: 0.0478 - val_loss: 0.0082 - val_mean_absolute_error: 0.0476\n",
      "Epoch 20/50\n",
      "0s - loss: 0.0076 - mean_absolute_error: 0.0476 - val_loss: 0.0082 - val_mean_absolute_error: 0.0476\n",
      "Epoch 21/50\n",
      "0s - loss: 0.0075 - mean_absolute_error: 0.0475 - val_loss: 0.0081 - val_mean_absolute_error: 0.0476\n",
      "Epoch 22/50\n",
      "0s - loss: 0.0075 - mean_absolute_error: 0.0473 - val_loss: 0.0081 - val_mean_absolute_error: 0.0476\n",
      "Epoch 23/50\n",
      "0s - loss: 0.0075 - mean_absolute_error: 0.0472 - val_loss: 0.0081 - val_mean_absolute_error: 0.0476\n",
      "Epoch 24/50\n",
      "0s - loss: 0.0075 - mean_absolute_error: 0.0471 - val_loss: 0.0081 - val_mean_absolute_error: 0.0476\n",
      "Epoch 25/50\n",
      "0s - loss: 0.0075 - mean_absolute_error: 0.0470 - val_loss: 0.0080 - val_mean_absolute_error: 0.0476\n",
      "Epoch 26/50\n",
      "0s - loss: 0.0074 - mean_absolute_error: 0.0469 - val_loss: 0.0080 - val_mean_absolute_error: 0.0477\n",
      "Epoch 27/50\n",
      "0s - loss: 0.0074 - mean_absolute_error: 0.0469 - val_loss: 0.0080 - val_mean_absolute_error: 0.0477\n",
      "Epoch 28/50\n",
      "0s - loss: 0.0074 - mean_absolute_error: 0.0468 - val_loss: 0.0080 - val_mean_absolute_error: 0.0477\n",
      "Epoch 29/50\n",
      "0s - loss: 0.0074 - mean_absolute_error: 0.0468 - val_loss: 0.0079 - val_mean_absolute_error: 0.0478\n",
      "Epoch 30/50\n",
      "0s - loss: 0.0074 - mean_absolute_error: 0.0467 - val_loss: 0.0079 - val_mean_absolute_error: 0.0478\n",
      "Epoch 31/50\n",
      "0s - loss: 0.0073 - mean_absolute_error: 0.0467 - val_loss: 0.0079 - val_mean_absolute_error: 0.0478\n",
      "Epoch 32/50\n",
      "0s - loss: 0.0073 - mean_absolute_error: 0.0467 - val_loss: 0.0078 - val_mean_absolute_error: 0.0479\n",
      "Epoch 33/50\n",
      "0s - loss: 0.0073 - mean_absolute_error: 0.0466 - val_loss: 0.0078 - val_mean_absolute_error: 0.0479\n",
      "Epoch 34/50\n",
      "0s - loss: 0.0073 - mean_absolute_error: 0.0466 - val_loss: 0.0078 - val_mean_absolute_error: 0.0479\n",
      "Epoch 35/50\n",
      "0s - loss: 0.0073 - mean_absolute_error: 0.0466 - val_loss: 0.0078 - val_mean_absolute_error: 0.0479\n",
      "Epoch 36/50\n",
      "0s - loss: 0.0072 - mean_absolute_error: 0.0466 - val_loss: 0.0077 - val_mean_absolute_error: 0.0479\n",
      "Epoch 37/50\n",
      "0s - loss: 0.0072 - mean_absolute_error: 0.0465 - val_loss: 0.0077 - val_mean_absolute_error: 0.0479\n",
      "Epoch 38/50\n",
      "0s - loss: 0.0072 - mean_absolute_error: 0.0465 - val_loss: 0.0077 - val_mean_absolute_error: 0.0479\n",
      "Epoch 39/50\n",
      "0s - loss: 0.0072 - mean_absolute_error: 0.0465 - val_loss: 0.0076 - val_mean_absolute_error: 0.0479\n",
      "Epoch 40/50\n",
      "0s - loss: 0.0071 - mean_absolute_error: 0.0465 - val_loss: 0.0076 - val_mean_absolute_error: 0.0480\n",
      "Epoch 41/50\n",
      "0s - loss: 0.0071 - mean_absolute_error: 0.0464 - val_loss: 0.0076 - val_mean_absolute_error: 0.0480\n",
      "Epoch 42/50\n",
      "0s - loss: 0.0071 - mean_absolute_error: 0.0464 - val_loss: 0.0075 - val_mean_absolute_error: 0.0480\n",
      "Epoch 43/50\n",
      "0s - loss: 0.0070 - mean_absolute_error: 0.0464 - val_loss: 0.0075 - val_mean_absolute_error: 0.0480\n",
      "Epoch 44/50\n",
      "0s - loss: 0.0070 - mean_absolute_error: 0.0464 - val_loss: 0.0074 - val_mean_absolute_error: 0.0480\n",
      "Epoch 45/50\n",
      "0s - loss: 0.0069 - mean_absolute_error: 0.0464 - val_loss: 0.0073 - val_mean_absolute_error: 0.0479\n",
      "Epoch 46/50\n",
      "0s - loss: 0.0069 - mean_absolute_error: 0.0464 - val_loss: 0.0073 - val_mean_absolute_error: 0.0478\n",
      "Epoch 47/50\n",
      "0s - loss: 0.0069 - mean_absolute_error: 0.0464 - val_loss: 0.0072 - val_mean_absolute_error: 0.0477\n",
      "Epoch 48/50\n",
      "0s - loss: 0.0068 - mean_absolute_error: 0.0464 - val_loss: 0.0071 - val_mean_absolute_error: 0.0476\n",
      "Epoch 49/50\n",
      "0s - loss: 0.0068 - mean_absolute_error: 0.0464 - val_loss: 0.0070 - val_mean_absolute_error: 0.0476\n",
      "Epoch 50/50\n",
      "0s - loss: 0.0067 - mean_absolute_error: 0.0465 - val_loss: 0.0070 - val_mean_absolute_error: 0.0475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ec03da0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Train model\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "mean, std = np.mean(data['observations'], axis=0), np.std(data['observations'], axis=0) + 1e-6\n",
    "\n",
    "observations_dim = env.observation_space.shape[0]\n",
    "actions_dim = env.action_space.shape[0]\n",
    "num_inputs = observations_dim\n",
    "num_outputs = actions_dim\n",
    "\n",
    "###Pick out the model here that you will use##\n",
    "model = baseline_model()\n",
    "#model = awesome_model()\n",
    "\n",
    "x, y = shuffle(data['observations'], data['actions'].reshape(-1, actions_dim))\n",
    "model.fit(x, y,\n",
    "          validation_split=0.1,\n",
    "          batch_size=256,\n",
    "          nb_epoch=epochs,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model and save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 29.87it/s]\n"
     ]
    }
   ],
   "source": [
    "returns = []\n",
    "observations = []\n",
    "actions = []\n",
    "steps_numbers = []\n",
    "\n",
    "for i in tqdm.tqdm(range(num_rollouts)):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    totalr = 0.\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        action = model.predict(obs[None, :])\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        totalr += r\n",
    "        steps += 1\n",
    "        if render_us:\n",
    "            env.render()\n",
    "        if steps >= max_steps:\n",
    "            break\n",
    "    steps_numbers.append(steps)\n",
    "    returns.append(totalr)\n",
    "\n",
    "our_net_data = {'observations': np.array(observations),\n",
    "                'actions': np.array(actions),\n",
    "                'returns': np.array(returns),\n",
    "                'steps': np.array(steps_numbers)}\n",
    "\n",
    "pickle.dump(our_net_data, open(our_data_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing experiment Reacher-v1\n",
      "                    expert  imitation\n",
      "mean reward      -3.521681 -12.831075\n",
      "pct full rollout  1.000000   1.000000\n",
      "std reward        1.508024   5.372770\n"
     ]
    }
   ],
   "source": [
    "###analyze single\n",
    "their = pickle.load(open(their_data_path, 'rb'))\n",
    "our = pickle.load(open(our_data_path, 'rb'))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'expert': one_data_table_stats(their),\n",
    "    'imitation': one_data_table_stats(our)\n",
    "})\n",
    "\n",
    "print (\"Analyzing experiment \" + envname)\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View examples of the expert policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-14 11:49:35,336] Making new env: Reacher-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rollouts from data:  5\n",
      "Start rollout  0\n",
      "Max timestep reached\n",
      "Start rollout  1\n",
      "Max timestep reached\n",
      "Start rollout  2\n",
      "Max timestep reached\n",
      "Start rollout  3\n",
      "Max timestep reached\n",
      "Start rollout  4\n",
      "Max timestep reached\n"
     ]
    }
   ],
   "source": [
    "view_data(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View examples of the imitated policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-14 11:49:39,655] Making new env: Reacher-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rollouts from model:  5\n",
      "Start rollout  0\n",
      "Max timestep reached\n",
      "Start rollout  1\n",
      "Max timestep reached\n",
      "Start rollout  2\n",
      "Max timestep reached\n",
      "Start rollout  3\n",
      "Max timestep reached\n",
      "Start rollout  4\n",
      "Max timestep reached\n"
     ]
    }
   ],
   "source": [
    "view_model(model,5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
